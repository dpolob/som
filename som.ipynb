{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python notebook for SOM\n",
    "\n",
    "### Diego Polo 3/5/2020\n",
    "Inpiration taken from\n",
    "http://www.ai-junkie.com/ann/som/som1.html\n",
    "https://visualstudiomagazine.com/articles/2019/01/01/self-organizing-maps-python.aspx\n",
    "http://blog.yhat.com/posts/self-organizing-maps-2.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(3, 100)\n"
    }
   ],
   "source": [
    "## Dataset\n",
    "\n",
    "#    Data will be a collection of random colours, so first we’ll artificially create a dataset of 100.\n",
    "#    Each colour is a 3D vector representing R, G and B values\n",
    "\n",
    "import numpy as np\n",
    "raw_data = np.random.randint(0,255, size=(3,100))\n",
    "print(raw_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setup\n",
    "\n",
    "network_dimensions = np.array([5, 5])\n",
    "n_iterations = 2000\n",
    "init_learning_rate = 0.01\n",
    "\n",
    "m = raw_data.shape[0]\n",
    "n = raw_data.shape[1]\n",
    "\n",
    "# weight matrix (i.e. the SOM) needs to be one m-dimensional vector for each neuron in the SOM\n",
    "# in this case, a matrix of 5x5 but and each element is an array of 3 dimensions (m)\n",
    "net = np.random.random((network_dimensions[0], network_dimensions[1], m))\n",
    "\n",
    "# initial neighbourhood radius\n",
    "init_radius = max(network_dimensions[0], network_dimensions[1]) / 2\n",
    "\n",
    "# radius decay parameter\n",
    "time_constant = n_iterations / np.log(init_radius)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Normalisation\n",
    "\n",
    "# Another detail to discuss at this point is whether or not we normalise our dataset.\n",
    "# First of all, SOMs train faster (and “better”) if all our values are between 0 and 1.\n",
    "# This is to avoid one of our dimensions “dominating” the others in the learning process.\n",
    "\n",
    "# we want to keep a copy of the raw data for later\n",
    "data = raw_data\n",
    "\n",
    "normalise_data = True\n",
    "normalise_by_column = True # no need as all colums have same range\n",
    "\n",
    "# check if data needs to be normalised\n",
    "if normalise_data:\n",
    "    if normalise_by_column:\n",
    "        # normalise along each column\n",
    "        col_maxes = raw_data.max(axis=0)\n",
    "        data =   raw_data / col_maxes[np.newaxis, :]\n",
    "    else:\n",
    "        # normalise entire dataset\n",
    "        data = raw_data / data.max()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Learning\n",
    "\n",
    "# decay the SOM parameters\n",
    "# We want to decay the learning rate over time to let the SOM “settle” on a solution.\n",
    "\n",
    "# What we also decay is the neighbourhood radius, which defines how far we search for 2D neighbours when updating vectors in the SOM. \n",
    "# We want to gradually reduce this over time, like the learning rate.\n",
    "\n",
    "# The functions to decay the radius and learning rate use exponential decay: σt = σ0·exp(-t/λ)\n",
    "# Where λ is the time constant (which controls the decay) and σ is the value at various times t.\n",
    "\n",
    "def decay_radius(initial_radius, iteration, lambdaa):\n",
    "    return initial_radius * np.exp(-iteration / lambdaa)\n",
    "\n",
    "def decay_learning_rate(initial_learning_rate, iteration, n_iterations):\n",
    "    return initial_learning_rate * np.exp(-iteration / n_iterations)\n",
    "\n",
    "\n",
    "\n",
    "# Find the neuron in the SOM whose associated 3D vector is closest to our chosen 3D colour vector.\n",
    "# At each step, this is called the Best Matching Unit (BMU)\n",
    "#     For that to work we need a function to find the BMU. It need to iterate through each neuron in the SOM, measure its Euclidean distance to our input vector and return the one that’s closest. Note the implementation trick of not actually measuring Euclidean distance, but the squared Euclidean distance, thereby avoiding an expensive square root computation.\n",
    "\n",
    "def find_bmu(t, net):\n",
    "    \"\"\"\n",
    "    Find the best matching unit for a given vector, t, in the SOM\n",
    "    Returns: a (bmu, bmu_idx) tuple where bmu is the high-dimensional BMU\n",
    "                and bmu_idx is the index of this vector in the SOM\n",
    "    \"\"\"\n",
    "    min_dist = np.Infinity\n",
    "    for x in range(net.shape[0]):\n",
    "        for y in range(net.shape[1]):\n",
    "            w = net[x, y, :]\n",
    "            # calculate kinda Euclidean distance\n",
    "            dist = np.sum((w - t)**2)\n",
    "            if dist < min_dist:\n",
    "                min_dist = dist\n",
    "                x_min = x\n",
    "                y_min = y\n",
    "    # get the vector corresponding to x_min and y_min\n",
    "    return (net[x, y, :], x_min, y_min)\n",
    "\n",
    "# Move the BMU’s 3D weight vector closer to the input vector in 3D space\n",
    "# The formula to update the BMU’s 3D vector is: Wt+1 = Wt + L(Vt-Wt)\n",
    "# That is to say, the new weight vector will be the current vector plus the difference between the input vector V\n",
    "# and the weight vector, multiplied by a learning rate L at time t\n",
    "\n",
    "# We also identify all the neurons in the SOM that are closer in 2D space than our current radius, and also move them closer to the input vector.\n",
    "# The difference is that the weight update will be proportional to their 2D distance from the BMU.\n",
    "# One last thing to note: this proportion of 2D distance isn’t uniform, it’s Gaussian.\n",
    "# Concretely, this is the equation we’ll use to calculate the influence i\n",
    "# It = exp(-d²/2σt²)\n",
    "# where d is the 2D distance and σ is the current radius of our neighbourhood.\n",
    "\n",
    "def influence(dist, radius):\n",
    "    return np.exp(-dist**2/(2*radius**2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeating step n_iterations\n",
    "for iteration in range(n_iterations):\n",
    "    # select a random value of the input vector\n",
    "    t = data[:, np.random.randint(data.shape[1])].flatten()\n",
    "    # get bmu and coordinates\n",
    "    bmu, x_min, y_min = find_bmu(t, net)\n",
    "\n",
    "\n",
    "    r = decay_radius(init_radius, iteration, lambdaa=0.01)\n",
    "    l = decay_learning_rate(init_learning_rate, iteration, n_iterations)\n",
    "    \n",
    "    for x in range(net.shape[0]):\n",
    "        for y in range(net.shape[1]):\n",
    "            w = net[x, y, :]\n",
    "            dist = (x - x_min)**2 + (y - y_min)**2\n",
    "            if dist < r**2:\n",
    "                w = w + l*(t-w)*influence(dist, r)\n",
    "                net[x, y, :] = w\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[[0.72728219, 0.20739531, 0.96658156],\n        [0.08983801, 0.27354102, 0.76868801],\n        [0.13332453, 0.14685405, 0.52532098],\n        [0.71630857, 0.18671411, 0.98179614],\n        [0.04658627, 0.78201806, 0.05672755]],\n\n       [[0.85920216, 0.59726002, 0.83412967],\n        [0.31580425, 0.39927502, 0.32838809],\n        [0.94191924, 0.46237571, 0.48703585],\n        [0.96044165, 0.45843714, 0.29769602],\n        [0.26317914, 0.01115242, 0.58291375]],\n\n       [[0.79922143, 0.24214217, 0.49953977],\n        [0.38035604, 0.49101567, 0.0690822 ],\n        [0.73985218, 0.73624268, 0.97836179],\n        [0.74515441, 0.72173049, 0.35974995],\n        [0.98955662, 0.67363347, 0.66830874]],\n\n       [[0.30856836, 0.31236597, 0.89676327],\n        [0.68727367, 0.29189318, 0.30592643],\n        [0.95853408, 0.10892847, 0.75139267],\n        [0.80442488, 0.76829684, 0.80743147],\n        [0.52234568, 0.88208081, 0.27473275]],\n\n       [[0.90393091, 0.3613171 , 0.19527785],\n        [0.55961507, 0.3956753 , 0.09753218],\n        [0.23715905, 0.95852851, 0.25134544],\n        [0.82147745, 0.8123865 , 0.53348289],\n        [0.7541089 , 0.42397966, 0.71270943]]])"
     },
     "metadata": {},
     "execution_count": 51
    }
   ],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python36964bitcab72b4e38e949d2bc33831970476da8",
   "display_name": "Python 3.6.9 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}